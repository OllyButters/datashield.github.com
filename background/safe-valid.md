---
layout: page
title: "Does DataSHIELD provide a safe and valid solution?"
description: ""
---
{% include JB/setup %}

The validity of adopting a DataSHIELD approach to pooled analysis when ethico-legal constraints prohibit the release of individual level data from at least one of the collaborating studies depends on two preconditions. 

1. The pooled analysis that is required must be able to be carried out using a parallelized approach based on a series of local analyses that can be linked together by passing analytic instructions and summary statistics between a single AC and a series of DCs. 
2. Neither the analytic instructions nor the summary statistics must carry information that could be viewed as being equivalent to individual-level data and might therefore be sensitive to, or might reveal the identity of, individual study participants.

Precondition 1 is fulfilled by virtually all pooled analyses of horizontally partitioned data (i.e. when each study contains all of the required information but on a different set of individuals ([Karr et al., 2007](/references), [Wolfson et al., 2010](/references))). Precondition 2 must be considered more carefully and there are two ways in which it could be violated. First, if the nature of the analysis itself means that the summary statistics might convey sensitive or identifying information. If the model can be constructed as a simple GLM this will not generally be a problem, because the summary statistics typically consist of a low dimensional ‘score vector’ and a low dimensional ‘information matrix’ (see box for a typical examples) that contain no information that can possibly be related to an individual subject. Here ‘low dimensionall’ implies that the number of elements in the vector (or equivalently, the number of rows in the matrix) is markedly less than the number of subjects in the study (see box). It is relevant to note that the elements in the vector and matrix are entirely unaffected by the order in which subjects appear in the data set. This means that even if the vector and matrix did contain sensitive or identifying information – which they do not – it would not be easy to determine the participant to which that information pertained. Equivalently, the new guess at the ultimate result(s) and the analytic instructions that are sent from the AC to each DC at the start of every new step convey no sensitive or identifying information. Finally, although the summary statistics that are sent back at the end of each step change from iteration to iteration, their basic form remains unchanged and the changing values simply track the gradual refinement of the analytic guess and so there is no sense in which by combining information from multiple iterations a ‘hacker’ could infer additional knowledge that could be sensitive or identifying.

![DataSHIELD](/images/Background%20and%20MethodologyPic1.png "DataSHIELD")

On the other hand, if the analysis demands a model that is more complex than a conventional GLM, for example a model containing many random effects, it is conceivable that the required summary statistics might enable information to be linked to an individual. Consequently, at this stage in its development, DataSHIELD is solely recommended for use with conventional GLMs and with models that would be viewed as acceptable for a standard SLMA analysis. Additional theoretical work is currently being undertaken to explore when more complex models - for example those containing random effects - may and may not safely be used.

In relation to its information content, any analysis that is currently viewed as being ethico-legally acceptable if undertaken as part of a conventional SLMA should be viewed as equally acceptable under DataSHIELD. This is because the type of information that is transmitted is the same. The only procedural difference is that, under DataSHIELD, the AC controls the analysis rather than the local statistician at each study. But this highlights the second way in which precondition 2 could potentially be violated. Namely, a malevolent statistician might deliberately set out to undertake a series of analyses that are jointly identifying. For example , this might represent a form of residual disclosure ([Gomatam et al., 2005](/references)). If a target subject in a particular study is known to be aged exactly 17 years and 312 days and one wants to know whether he has schizophrenia, one might ask two sets of apparently innocuous questions: 

1. how many people in the data set are aged less than 17 years and 312 days and what is the prevalence of schizophrenia in that subset; 
2. how many people in the data set are aged less than or equal to 17 years and 312 days and what is the prevalence of schizophrenia in that subset. Assuming there are no other participants who are precisely the same age, the number of people will increase by one in the second setting. That being the case, if the second prevalence is lower than the first the target subject does not have schizophrenia, if it is higher he/she does have schizophrenia. Thus, when asked in combination, it is possible for individually innocuous questions to reveal sensitive information. This may occur accidently or deliberately and it is not in any sense specific to the use of DataSHIELD – residual disclosure can be problematic in any data set, but as DataSHIELD provides the AC with the power to ask analytic questions on remote data servers, it is important that it implemented in a manner that minimises the opportunity for such malevolent interrogation to occur.

In light of these potential (though remote) concerns, and other ethico-legal and scientific considerations, it is recommended that DataSHIELD should only be implemented if a number of safeguards are in place:

* The need (or not) to seek ethical approval to use DataSHIELD should be explored with the ethical and/or other governance committees that oversee the collaborating studies

* All participating scientists and statisticians should sign a formal confidentiality agreement that guarantees that they will not misuse DataSHIELD by trying to identify any participant in the collaborating studies or to assign a value to a specific variable in any participant.

* All information passed to and from the data computer (DC) at each study should be permanently recorded (at the DC). This will mean that in the unlikely event that a breach of security should occur – either by accident or by design – it can be identified and/or investigated post hoc.

* No new class of model should be fitted using DataSHIELD until the information content of its summary statistics has been comprehensively explored and is thoroughly understood.

* From a scientific perspective, any data to be pooled under DataSHIELD must be adequately harmonized([Fortier et al., 2010](/references)) so that a pooled analysis is both valid and meaningful.

For the future, the DataSHIELD team aims to develop middleware that will monitor, scrutinise and interpret all incoming and outgoing information flows and will block and identify/record any request, or series of requests, that might – either by accident or by design – generate potentially disclosive information. In the short term, before such middleware is available, it is recommended that additional security might be achieved by having one or more statisticians from the participating studies in regular contact with the statistician at the analysis centre so that the progress of the analysis can be monitored. From the perspective of good collaborative science, it might be useful for all of the analysis to be undertaken with real-time communication between the researchers from more than one of the studies. But, from the specific perspective of security, provided all information flowing in and out of each DC is recorded locally (as under safeguard 3), the AC statistician can safely undertake the analysis alone (which may be more efficient) and the security of that analysis can be checked as and when the researchers at each DC wish. Particularly careful scrutiny will be warranted where an analysis involves sub-setting of data (see above).

Having said all of this, it must be re-emphasised that analogous concerns already apply across a wide variety of settings where third parties require access to secure data (as now required by most major funders); regardless whether pooling is required or not. In other words, these issues are certainly not specific to DataSHIELD, it is just that it is important that if DataSHIELD is to be used it must be implemented in a manner that minimises such risks. Such a philosophy has already been adopted in relation to providing access to biomedical data from the British 1958, 1970 and Millennium Birth Cohorts by the National Access Committee (see http://www2.le.ac.uk/projects/birthcohort ). Because they contain all of the people born in Great Britain during one week in 1958 and in 1970, the data from the first two of these studies are particularly susceptible to a variety of attacks from malevolent users, and yet the 1958 Birth Cohort has successfully functioned as a premier provider of large-scale anonymised individual-level biomedical data since 2003. In order to minimise the risk of misuse, all users are required to sign a confidentiality guarantee (see [section 3g ‘Security’](http://www2.le.ac.uk/projects/birthcohort/document-downloads/POLICY.DOCUMENT.120609.pdf)) and data sets are constructed in ways that minimise the risk that someone might consider trying to misuse the data (for example, every time a new data award is made, data files are re-indexed with new IDs that have not previously been used, and the data sets are reordered – these manoeuvres greatly reduce the risk that two different users will link information together that has not been countenanced by the access committee and might therefore present a particular security risk). But the access committee is entirely realistic; it is recognised that the risk can never be entirely negated. Nevertheless, despite the fact that hundreds of research groups across the world have used the data over the last 7 years, the handful of problems that have come to light have all related to naive users mistakenly – and unsuccessfully - trying to do things that they were not aware that they were not allowed to do.

In light of the foregoing, and provided it is implemented carefully and with the stated safeguards, it is difficult to see how DataSHIELD could significantly increase the opportunities for deliberately misusing data for those few miscreants that intend to do so regardless. That being the case, it is not at all unreasonable to view the ethico-legal implications of DataSHIELD to be precisely analogous to those of a conventional SLMA. The type of information that is transmitted in both settings (analytic instructions and non-sensitive summary statistics) is effectively equivalent. And, in both cases individual-level data never leave their host study. This implies that if an ethics or governance committee interprets the wording of the ethico-legal documentation of a particular study as prohibiting that study from sharing the results of study-specific analyses with third parties as part of an SLMA, that study should also be viewed as being unsuitable for DataSHIELD. On the other hand, if the ethico-legal documentation is judged to indicate that a study is able to participate in a conventional collaborative meta-analysis (SLMA), then logically, DataSHIELD should also be viewed as acceptable - unless, there is a specific (and unusual) reason to believe that there is a significant risk that DataSHIELD will deliberately be misused in a manner that will not be prevented by, or detected under, the protocol safeguards described above (no such approach has, as yet, been identified).  