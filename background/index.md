---
layout: page
title: "Background and Methodology"
description: ""
---
{% include JB/setup %}

## The need for pooled data analysis

Contemporary exploration of the biomedical and social determinants of health and disease often requires scientists to identify and quantify the relatively weak effect of one or more factors of primary relevance (e.g. a small number of specified genes and environmental determinants) that are shrouded behind a smoke-screen of other factors that are causally important but not of substantive interest in the current study (i.e. all of the other determinants that influence the trait of interest). This is akin to fine-tuning a radio – it demands that the signal to noise ratio is large enough to allow clear reception. In the biomedical setting this often means that a very large number of subjects must be studied ([Burton et al., 2009](/references)). At the same time, studies must invest in measurements that are of ‘high quality’([Wong et al., 2003](/references)) and this can be very expensive when tens or even hundreds of thousands of participants are to be measured. This places a pragmatic limit on the total number of subjects that any single study can enrol and many research questions of undoubted scientific interest simply cannot be answered on the basis of data from one study alone. [Read More....](/background/need-pooled.html)

## Study-level and individual-level meta-analysis (SLMA&ILMA) 

The barriers to individual-level data access described in section 1 cannot simply be ignored. They reflect real societal concerns about privacy, confidentiality and the ownership and exploitation of scientific data and intellectual property. Fortunately, however, approaches have been developed that allow pooled analyses to be carried out that are felt not to violate ethico-legal and governance stipulations. For example, much of the recent progress in our understanding of the genetic variants that cause a range of common chronic diseases (e.g. cancer, coronary artery disease, diabetes, asthma and arthritis ([Hindorff et al., 2009](/references)) has arisen from pooled analyses where the analysis is based on what may be called ‘Study-Level Meta-Analysis’ (SLMA)([Wolfson et al., 2010](/references)) or aggregate-level analysis ([Sutton et al., 2008](/references)). Here the association between each gene and the disease of interest is first estimated in each study separately. [Read More....](/background/lma.html)

## The need for DataSHIELD

Given the potential to use either SLMA or ILMA why does pooled data analysis still present unresolved challenges? In fact, these challenges are implicit to the realistic responses to two key questions: 

1. Why have clinical genomicists and genetic epidemiologists used SLMA so much more widely than individual-level meta-analysis in the recent past? ([Hindorff et al., 2009](/references), [Fortier et al., 2010](/references))
2. Why should we not continue to use SLMA in preference to ILMA in the future?

The answer to the first question follows directly from the description of SLMA above: unlike ILMA, SLMA avoids the need to pass any individual level data from the collaborating studies to the analysis centre. In consequence, many ethical committees, study oversight committees and scientific advisory boards have been happy to conclude that even if ethico-legal restrictions prohibit the transmission of individual-level data to third parties (thereby preventing conventional ILMA) it is entirely acceptable for study investigators to analyse their own data and then to pass the summary statistics generated by those analyses to an analysis centre to provide a foundation for a pooled SLMA. [Read More....](/background/need-datashield.html)

## DataSHIELD: how does it work?

The figure below illustrates the basic IT infrastructure that underpins DataSHIELD. It describes a hypothetical implementation based on a pooled analysis involving data from six studies. Most crucially, all of the individual-level data that provide the basis of the analysis remain securely on ’data computers’ (DCs) at their home bases. An additional computer is identified as the ‘analysis computer’ (AC). This is the computer on which the statistician will type the commands to enact and control the pooled analysis. In actuality, the analysis computer may be based at the same centre as one of the data computers or it may be one of the data computers. [Read More....](/background/datashield.html)

## Does DataSHIELD provide a safe and valid solution?

The validity of adopting a DataSHIELD approach to pooled analysis when ethico-legal constraints prohibit the release of individual level data from at least one of the collaborating studies depends on two preconditions. (1) The pooled analysis that is required must be able to be carried out using a parallelized approach based on a series of local analyses that can be linked together by passing analytic instructions and summary statistics between a single AC and a series of DCs. (2) Neither the analytic instructions nor the summary statistics must carry information that could be viewed as being equivalent to individual-level data and might therefore be sensitive to, or might reveal the identity of, individual study participants. [Read More....](/background/safe-valid.html)