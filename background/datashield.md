---
layout: page
title: "DataSHIELD: how does it work?"
description: ""
---
{% include JB/setup %}

<ul class="pager">
  <li class="previous">
    <a href="/background">&larr; Background</a>
  </li>
</ul>

The figure below illustrates the basic IT infrastructure that underpins DataSHIELD. It describes a hypothetical implementation based on a pooled analysis involving data from six studies. Most crucially, all of the individual-level data that provide the basis of the analysis remain securely on ’data computers’ (DCs) at their home bases. An additional computer is identified as the ‘analysis computer’ (AC). This is the computer on which the statistician will type the commands to enact and control the pooled analysis. In actuality, the analysis computer may be based at the same centre as one of the data computers or it may be one of the data computers– but it is simpler to envisage if it is assumed (as in the figure) that the AC is independent of all of the DCs.

Given the IT configuration implied by the figure, a pooled analysis based on a **conventional ILMA** would require that the data from each of the DCs were first transmitted to the AC. The AC would therefore host the data for analysis as well as providing the point of entry for the instructions of the statistician. Under **DataSHIELD**, in contrast, the data remain on the DCs, and the AC serves primarily as an entry port for the analytic instructions and as a mathematical platform for integrating the analytic output. This is rendered possible because the analysis itself is ‘parallelized’. That is, rather than analysing all of the data at once - as in a conventional analysis - the data from each study are analysed separately but contemporaneously.

![DataSHIELD](/images/HowItWorksPicCompact.png "DataSHIELD")

The above figure shows and overview of a DataSHIELD process. In this graphical representation an iterative analysis is illustrated for one study only for the sake of concision but in reality the same process is trigged simultaneously in all four studies.
(0) preliminary and prerequisite step, the Data to be shared are transferred to the data owner’s opal environment (the shared data admin area); 
(1) the user sends a command to connect to the collaborating servers; 
(2) the user requests the data to analyse (part or all the shared data); 
(3) the requested data are transferred to the data owner’s R environment (the shared data analysis area); 
(4) the user sends an analysis command (e.g. a model to fit or a summary or graph request); 
(5) the command is executed within the data owner’s R environment; 
(6) Non-disclosive summary statistics are send back to the user; 
(7) the returned summary statistics are combined by the analysis computer; 
(8) In an iterative process, the updated coefficient estimates of the fitted model are sent back to the DCs for a new fit. 

Some DataSHIELD analyses are **simple one step procedures** –e.g. the function that creates a simple one-dimensional contingency table. This function first creates a local one-dimensional contingency table at each study (a server side procedure) and checks whether the table presents a potential disclosure risk (i.e. do any cells contain between 1 and 4 observations?). When the table for a given study is entirely valid, that table is returned directly to the AC. Studies with an invalid table return a 1 count for offending cells, and a zero for all other cells. Once all of this tabular information has been passed to the client side, a client side R function combines the valid data together to produce an aggregate table across all studies presenting valid data. These valid data are then presented as counts, row %, column % and global % for all valid studies combined and for individual studies alone. Data from studies with invalid tables are presented by individual study alone; this enables the analyst to identify categories that could be collapsed to enable analysis while preserving security. Others functions carry out more **complex iterative procedures**. These include the function datashield.glm() that fits generalized linear models (GLM) which enable a wide range of analyses to be undertaken. As indicated in the figure above, iterative procedures involve the transmission of analytic instructions and critical parameters from the AC to each of the DCs with the subsequent return of key summary statistics based on the specified analysis, from each DC server to the AC client. The AC then uses the summary statis tics to update the current value of the critical parameters and retransmits these updated parameters with further analytic instructions back to the DC servers. This repeated process continues until the summary statistics are effectively unchanged between two iterations and convergence is then said to have been achieved. Crucially, throughout the whole procedure, the individual level data never leave, and are never visible outside, the server on which they customarily reside. Furthermore, the analytic instructions, and critical parameters (e.g. the current optimal estimate of the vector of regression coefficients when fitting a GLM) carry no sensitive information from AC to DC. And, even more crucially, the summary statistics (typically, information matrices and score vectors) returned from each DC to the AC carry no information about individual participants and are therefore non-disclosive.

For most single step procedures, and for the broad class of mathematical models that are collectively known as generalized linear models (GLMs), the results that are ultimately obtained using the DataSHIELD approach described in the preceding paragraph are mathematically identical to those that would have been obtained if the required data from all of the collaborating studies had been placed into one large data file, and that file had then been analysed directly – i.e. a full conventional ILMA. Thus, DataSHIELD enables a pooled data analysis to be carried out as if one had full access to all of the individual-level data but, in reality, these data are held securely behind IT screens (dotted lines on figure). This approach is very useful because many of the analyses that are undertaken most commonly in biomedical and social research can easily be framed as GLMs. For example, this includes basic contingency table analysis and many foundation methods for quantitative variables (such as t-testing and basic analysis of variance). It also includes many of the most widely used classes of regression analysis (e.g. linear, logistic and Poisson regression) and several types of survival analysis.

But other approaches to pooled analysis can also benefit from implementation via DataSHIELD. For example, as described earlier, the fundamental inflexibility of traditional SLMA is that when the AC wishes to undertake a new analysis that had not previously been foreseen, the analytic pipeline is blocked while everybody waits for all of the collaborating studies to calculate the new summary statistics that are now required. But, if the SLMA is carried out under DataSHIELD, the AC is able to specify the additional analyses directly and so there is no need to wait. In fact an SLMA may be carried out under DataSHIELD as a single step procedure. The AC simply transmits an instruction to each DC (outward pointing arrows on the figure): “carry out the following analysis (the specific analysis that is required) and then return summary statistics that represent the final result of this analysis based solely on the data in your DC” (inward pointing arrows on the figure). The AC then takes the summary statistics from each study and combines them together in precisely the same way that a contemporary analysis centre would integrate the study-level results as part of a conventional SLMA. The key difference is that, under DataSHIELD, the AC controls the study-level (parallelized) analyses centrally (the local researchers need do nothing). This implies that a totally new analysis can then be carried out without having to wait for the local researchers at the studies to receive verbal or written instructions and then to undertake additional analyses themselves. DataSHIELD can therefore be used to make SLMA far more efficient. But, if one can easily replicate a full ILMA under DataSHIELD, there seems to be only limited value in using it solely to implement an SLMA.

<ul class="pager">
  <li class="previous">
    <a href="/background">&larr; Background</a>
  </li>
</ul>
